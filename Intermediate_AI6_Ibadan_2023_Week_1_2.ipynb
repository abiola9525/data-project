{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igTWb83CMnDg"
   },
   "source": [
    "**Weeks 1-2: Advanced Python Programming**\n",
    "\n",
    "\n",
    "*   Advanced Control Structures and Functions\n",
    "*   Object-Oriented Programming in Python\n",
    "*   Error Handling and Debugging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv-JZnGOO6Vl"
   },
   "source": [
    "**Advanced Control Structures and Functions**\n",
    "\n",
    "\n",
    "1. List, Set, and Dictionary Comprehensions for Data Manipulation\n",
    "2. Map, Filter, and Reduce Functions for Data Processing\n",
    "3. Generator Expressions for Large Datasets\n",
    "4. Asynchronous Programming for Data Processing\n",
    "5. Higher-Order Functions for Parallelization and Distributed Computing\n",
    "6. Functional Programming Paradigms for Data Pipelines\n",
    "7. Pandas and NumPy Libraries for Data Manipulation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhu53J3vQChG"
   },
   "source": [
    "**List, Set, and Dictionary Comprehensions for Data Manipulation**\n",
    "---\n",
    "\n",
    "\n",
    " *   List, set, and dictionary comprehensions are concise and readable ways to construct new sequences or data structures by transforming or filtering existing ones.\n",
    "*   This is important in data science and machine learning to manipulate large datasets efficiently. \n",
    "*   Comprehensions help to simplify code and reduce the number of lines of code required for data manipulation tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz9qCWHFUG3_"
   },
   "source": [
    "List, set, and dictionary are three common built-in data structures in Python, each with its own unique properties and use cases. Here are some differences between them:\n",
    "\n",
    "\n",
    "*   A list is an ordered collection of elements that can be of any type. \n",
    "\n",
    "*   Lists are mutable, meaning that their elements can be changed or modified.\n",
    "*   Elements in a list can be accessed using their index.\n",
    "*   Lists are created using square brackets [] and can contain duplicates.\n",
    "*   Lists are commonly used for sequences of values that need to be maintained in a specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9fVNT5NVh_P",
    "outputId": "202ed0d9-15b2-4df7-d839-f72c74cd1194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'orange', 'kiwi']\n"
     ]
    }
   ],
   "source": [
    "fruits = ['apple', 'banana', 'orange', 'kiwi']\n",
    "print(fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hI_ujMbMUts",
    "outputId": "92d840ec-f7e3-485f-8778-f110859c9b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 16, 36, 64, 100]\n"
     ]
    }
   ],
   "source": [
    "# List Comprehension \n",
    "# Create a list of squares of even numbers from 1 to 10\n",
    "squares_of_evens = [x**2 for x in range(1, 11) if x % 2 == 0]\n",
    "print(squares_of_evens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2reZJ9GUtte"
   },
   "source": [
    "**Set:**\n",
    "\n",
    "*   A set is an unordered collection of unique elements that can be of any type.\n",
    "*   Sets are mutable, meaning that their elements can be added or removed.\n",
    "Elements in a set cannot be accessed using their index.\n",
    "*   Sets are created using curly braces {} or the set() function and do not contain duplicates.\n",
    "*   Sets are commonly used to perform mathematical set operations, such as union, intersection, and difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xfpTC8mVoLI",
    "outputId": "d5880d7e-eb2a-4071-ec93-938123c01cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red', 'green', 'blue'}\n"
     ]
    }
   ],
   "source": [
    "colors = {'red', 'green', 'blue'}\n",
    "print(colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKCuiBk-REzG",
    "outputId": "9762ee01-f933-4ab8-d4d6-4c3d1a51e149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C', 'A', 'B'}\n"
     ]
    }
   ],
   "source": [
    "# Set Comprehension Example\n",
    "# Create a set of unique first letters of a list of names\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"Alice\"]\n",
    "first_letters = {name[0] for name in names}\n",
    "print(first_letters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fot2P2xsVIBt"
   },
   "source": [
    "**Dictionary:**\n",
    "\n",
    "\n",
    "*   A dictionary is an unordered collection of key-value pairs, where each key is unique and associated with a value.\n",
    "\n",
    "*   Dictionaries are mutable, meaning that their key-value pairs can be added or modified.\n",
    "*   Elements in a dictionary can be accessed using their keys.\n",
    "*   Dictionaries are created using curly braces {} or the dict() function and do not contain duplicates (only unique keys).\n",
    "*   Dictionaries are commonly used to represent mappings between different objects, such as a mapping between names and ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59Pimny5VtWm",
    "outputId": "d56aed33-2dab-4423-a21c-408eb709be24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': 25, 'Bob': 30, 'Charlie': 35}\n"
     ]
    }
   ],
   "source": [
    "ages = {'Alice': 25, 'Bob': 30, 'Charlie': 35}\n",
    "print(ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKiLIDFfRNmd",
    "outputId": "fcef9d6f-758d-4bf3-fe1d-235401fdeed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 3: 6, 5: 10, 7: 14, 9: 18}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary Comprehension\n",
    "# Create a dictionary with values doubled for all odd keys in a range\n",
    "doubled_odds = {x: x*2 for x in range(1, 11) if x % 2 != 0}\n",
    "print(doubled_odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNNMv_RkW49m"
   },
   "source": [
    "**Tuple:**\n",
    "\n",
    "\n",
    "*   A tuple is an ordered collection of elements that can be of any type, similar to a list.\n",
    "*   Tuples are immutable, meaning that their elements cannot be changed or modified after creation.\n",
    "*   Elements in a tuple can be accessed using their index, similar to a list.\n",
    "*   Tuples are created using parentheses () or the tuple() function and can contain duplicates.\n",
    "*   Tuples are commonly used to group related data together when you want to ensure that the values cannot be changed accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrUJwrCjXbZG"
   },
   "outputs": [],
   "source": [
    "# create a tuple of student names and their corresponding ages\n",
    "students = (('Alice', 25), ('Bob', 30), ('Charlie', 35))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ecn_XwYZX0sW",
    "outputId": "6c02e0ab-aa88-49ba-93d6-f2745719e8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "# print the first element of the first tuple\n",
    "print(students[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vtn48fHhX4Gf",
    "outputId": "25dc546b-514c-41de-817d-19ca04c7a285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# print the second element of the second tuple\n",
    "print(students[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmM5U2x9X8-Q",
    "outputId": "a4c77caa-4166-47ad-e973-40501e075606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice 25\n",
      "Bob 30\n",
      "Charlie 35\n"
     ]
    }
   ],
   "source": [
    "# loop through the tuples and print out each element\n",
    "for student in students:\n",
    "    name, age = student\n",
    "    print(name, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNbIA_RUYxQs"
   },
   "source": [
    "**Knowledge of List, Set and Dictionary on data frame**\n",
    "\n",
    "here's an example of how you could use lists, sets, and dictionaries with data imported to Pandas from an Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qABr70nLYuQ3",
    "outputId": "d57379df-726d-4691-887d-2f5d53f30394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['Unnamed: 0', 'PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "Unique Column Names: ['Pclass', 'Unnamed: 0', 'Sex', 'Age', 'SibSp', 'Survived', 'Parch', 'Fare', 'Ticket', 'Embarked', 'Cabin', 'Name', 'PassengerId']\n",
      "Column Types: {'Pclass': 'float64', 'Unnamed: 0': 'int64', 'Sex': 'object', 'Age': 'float64', 'SibSp': 'float64', 'Survived': 'float64', 'Parch': 'float64', 'Fare': 'float64', 'Ticket': 'object', 'Embarked': 'object', 'Cabin': 'object', 'Name': 'object', 'PassengerId': 'float64'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import data from an csv file\n",
    "df = pd.read_csv('tested.csv')\n",
    "\n",
    "# create a list of unique column names\n",
    "columns = list(df.columns)\n",
    "unique_columns = list(set(columns))\n",
    "\n",
    "# create a dictionary of column types\n",
    "types = {}\n",
    "for column in unique_columns:\n",
    "    types[column] = str(df[column].dtype)\n",
    "\n",
    "# print out the results\n",
    "print('Column Names:', columns)\n",
    "print('Unique Column Names:', unique_columns)\n",
    "print('Column Types:', types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfeytwL2jXY2"
   },
   "source": [
    "**Set Exercises**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given two sets, find their intersection.\n",
    "* Given two sets, find their union.\n",
    "* Given a set, check if it is a subset of another set.\n",
    "* Given a set of numbers, find the maximum value.\n",
    "* Given a set of strings, find the longest string.\n",
    "* Given two sets, find their symmetric difference.\n",
    "* Given a set, remove all even numbers from it.\n",
    "* Given a set, remove duplicates from it.\n",
    "* Given two sets, find their difference.\n",
    "* Given a set, check if it is empty.\n",
    "\n",
    "**List Exercises**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* Given a list, find its length.\n",
    "* Given a list, find the sum of all its elements.\n",
    "* Given a list, find the maximum value.\n",
    "* Given a list, find the minimum value.\n",
    "* Given a list, find the average value.\n",
    "* Given a list, find the median value.\n",
    "* Given a list, find the mode value.\n",
    "* Given a list, remove all even numbers from it.\n",
    "* Given a list, sort it in ascending order.\n",
    "* Given a list, reverse its order.\n",
    "\n",
    "**Dictionary Exercises**\n",
    "\n",
    "---\n",
    "* Given a dictionary, find its length.\n",
    "* Given a dictionary, find all its keys.\n",
    "* Given a dictionary, find all its values.\n",
    "* Given a dictionary, find the maximum value.\n",
    "* Given a dictionary, find the minimum value.\n",
    "* Given a dictionary, add a new key-value pair to it.\n",
    "* Given a dictionary, remove a key-value pair from it.\n",
    "* Given two dictionaries, combine them into a single dictionary.\n",
    "* Given a dictionary, sort it by its keys.\n",
    "* Given a dictionary, sort it by its values.\n",
    "\n",
    "**Tuple Exercises**\n",
    "\n",
    "---\n",
    "* Given a tuple, find its length.\n",
    "* Given a tuple, find the maximum value.\n",
    "* Given a tuple, find the minimum value.\n",
    "* Given a tuple, find the sum of all its elements.\n",
    "* Given a tuple, find the average value.\n",
    "* Given a tuple, find the median value.\n",
    "* Given a tuple, convert it to a list.\n",
    "* Given two tuples, concatenate them into a single tuple.\n",
    "* Given a tuple, reverse its order.\n",
    "* Given a tuple, check if it contains a specific element.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3sUP54UdnWn"
   },
   "source": [
    "**Map, filter, and reduce functions**\n",
    "---\n",
    "\n",
    "* Map, filter, and reduce functions are built-in functions in Python that allow you to perform operations on sequences or iterables. \n",
    "* This is important in data science and machine learning to process data quickly and efficiently. \n",
    "Map, filter, and reduce functions can be used to transform data, filter out unwanted elements, and aggregate data into a single value.\n",
    "* Map, filter, and reduce functions are commonly used in data science for data manipulation and processing. Here are some examples of how these functions can be applied in a data science context:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EL0bF-cdvNO"
   },
   "source": [
    "**Map Function for Data Transformation**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The map() function can be used to transform data in a DataFrame or Series. For example, let's say we have a DataFrame containing the heights of students in meters, and we want to convert the heights to feet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h66p78jYtOK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a DataFrame of heights in meters\n",
    "heights = pd.DataFrame({'height': [1.6, 1.7, 1.8, 1.9]})\n",
    "\n",
    "# use the map function to convert the heights to feet\n",
    "heights['height_feet'] = heights['height'].map(lambda x: x * 3.28084)\n",
    "\n",
    "# print out the results\n",
    "print(heights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOWWy8Rxd6vm"
   },
   "source": [
    "In the above example, we're using the map() function to apply the lambda x: x * 3.28084 function to each element in the height column of the heights DataFrame, and storing the results in a new column called height_feet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AbunQPGeF7f"
   },
   "source": [
    "**Filter Function for Data Filtering**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The filter() function can be used to filter data in a DataFrame or Series based on a condition. For example, let's say we have a DataFrame containing the ages of students, and we want to filter out students who are under 18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7FUAKzEeB8_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a DataFrame of student ages\n",
    "ages = pd.DataFrame({'age': [20, 17, 25, 16, 18]})\n",
    "\n",
    "# use the filter function to keep only students who are 18 or older\n",
    "ages_over_18 = ages[ages['age'].apply(lambda x: x >= 18)]\n",
    "\n",
    "# print out the results\n",
    "print(ages_over_18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK4W4Bz3eWhR"
   },
   "source": [
    "In the above example, we're using the apply() function to apply the lambda x: x >= 18 function to each element in the age column of the ages DataFrame, and using the filter() function to keep only the rows where the condition is True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27wR2of-efpz"
   },
   "source": [
    "**Reduce Function for Data Aggregation**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The reduce() function can be used to aggregate data in a DataFrame or Series. For example, let's say we have a DataFrame containing the heights of students in meters, and we want to find the average height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf8NBcDdeczl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# create a DataFrame of heights in meters\n",
    "heights = pd.DataFrame({'height': [1.6, 1.7, 1.8, 1.9]})\n",
    "\n",
    "# use the reduce function to find the average height\n",
    "average_height = reduce(lambda x, y: x + y, heights['height']) / len(heights)\n",
    "\n",
    "# print out the results\n",
    "print(average_height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex1LlEGRelk5"
   },
   "source": [
    "In the above example, we're using the reduce() function to apply the lambda x, y: x + y function to each element in the height column of the heights DataFrame, and then dividing the result by the length of the DataFrame to find the average height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkNA_3k8erxp"
   },
   "source": [
    "**Exercise for Class**\n",
    "\n",
    "---\n",
    "**Map Exercises**\n",
    "* Given a list of temperatures in Celsius, use the map() function to convert them to Fahrenheit.\n",
    "* Given a list of strings, use the map() function to convert them to lowercase.\n",
    "* Given a DataFrame of customer names, use the map() function to create a new column of initials.\n",
    "* Given a DataFrame of product prices, use the map() function to apply a discount of 10% to each price.\n",
    "* Given a list of numbers, use the map() function to round them to the nearest integer.\n",
    "* Given a list of tuples, use the map() function to create a new list of the first elements of each tuple.\n",
    "* Given a list of dictionary objects, use the map() function to create a new list of values for a specific key.\n",
    "* Given a DataFrame of dates, use the map() function to create a new column of day of week.\n",
    "* Given a list of strings, use the map() function to split each string into a list of words.\n",
    "* Given a DataFrame of product descriptions, use the map() function to create a new column of word counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgXgd069fSkE"
   },
   "source": [
    "**Filter Exercises**\n",
    "\n",
    "---\n",
    "* Given a list of numbers, use the filter() function to keep only the even numbers.\n",
    "* Given a list of strings, use the filter() function to keep only the strings containing the letter 'a'.\n",
    "* Given a list of dictionaries, use the filter() function to keep only the dictionaries where a specific key exists.\n",
    "* Given a list of tuples, use the filter() function to keep only the tuples where the first element is greater than the second element.\n",
    "* Given a list of numbers, use the filter() function to keep only the numbers greater than 10.\n",
    "* Given a list of strings, use the filter() function to keep only the strings longer than 10 characters.\n",
    "* Given a list of tuples, use the filter() function to keep only the tuples where both elements are positive.\n",
    "* Given a list of dictionaries, use the filter() function to keep only the dictionaries where all keys have values.\n",
    "* Given a list of strings, use the filter() function to keep only the strings that are palindromes.\n",
    "* Given a list of numbers, use the filter() function to keep only the numbers that are perfect squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4OGkyUjfr0u"
   },
   "source": [
    "**Reduce Exercises**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* Given a list of numbers, use the reduce() function to find the sum of the numbers.\n",
    "* Given a list of strings, use the reduce() function to concatenate the strings.\n",
    "* Given a list of tuples, use the reduce() function to find the product of the first elements.\n",
    "* Given a list of numbers, use the reduce() function to find the maximum value.\n",
    "* Given a list of strings, use the reduce() function to find the longest string.\n",
    "* Given a list of tuples, use the reduce() function to find the minimum value of the second elements.\n",
    "* Given a list of numbers, use the reduce() function to find the product of the numbers.\n",
    "* Given a list of strings, use the reduce() function to find the total length of all the strings.\n",
    "* Given a list of tuples, use the reduce() function to find the sum of the second elements where the first elements are even.\n",
    "* Given a list of dictionaries, use the reduce() function to find the dictionary with the maximum value for a specific key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyEuKm2IkvxV"
   },
   "source": [
    "**Generator Expressions for Large Datasets:**\n",
    "---\n",
    "\n",
    "\n",
    "* Generator expressions are a memory-efficient way to process large datasets, one element at a time. \n",
    "* This is important in data science and machine learning when dealing with datasets that are too large to fit into memory all at once. \n",
    "* Generator expressions can be used to process data on-the-fly, without loading the entire dataset into memory.\n",
    "\n",
    "To expand on this, a generator expression is similar to a list comprehension in syntax, but it returns a generator object instead of a list. A generator object is an iterator that generates the next element of a sequence on-the-fly, instead of storing all the elements in memory at once. This makes generator expressions very useful for processing large datasets, as they allow you to iterate over the data one element at a time, without loading the entire dataset into memory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GNGEVy8lXxd"
   },
   "source": [
    "**Reading Large CSV Files**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "When reading large CSV files, you can use a generator expression to read one line at a time and process it on-the-fly. This can be much more memory-efficient than reading the entire file into memory at once. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lczfgq3NrxVN"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2622244900.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    # process the chunk here\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# use a generator expression to read one line at a time\n",
    "gen = (chunk for chunk in pd.read_csv('large_file.csv', chunksize=1000))\n",
    "\n",
    "# iterate over the chunks and process them on-the-fly\n",
    "for chunk in gen:\n",
    "    # process the chunk here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOoHhiyPrwo8"
   },
   "source": [
    "**Processing Large Datasets**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Generator expressions can be used to process large datasets one element at a time, without loading the entire dataset into memory. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEoflGXjr4SN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# use a generator expression to process the data on-the-fly\n",
    "gen = (x**2 for x in pd.read_csv('large_file.csv')['column'])\n",
    "\n",
    "# iterate over the generator and process the data one element at a time\n",
    "for x_squared in gen:\n",
    "    # process the data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDoyby6PsN1Z"
   },
   "source": [
    "**Filtering Large Datasets**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Generator expressions can be used to filter large datasets on-the-fly, without loading the entire dataset into memory. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkTdCithsTaD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# use a generator expression to filter the data on-the-fly\n",
    "gen = (x for x in pd.read_csv('large_file.csv')['column'] if x > 0)\n",
    "\n",
    "# iterate over the generator and process the filtered data one element at a time\n",
    "for x_filtered in gen:\n",
    "    # process the filtered data here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LQtmbRu7kPb"
   },
   "source": [
    "**Exercises for Generator Expressions for Large Datasets:**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given a large CSV file with millions of rows, use a generator expression to read the file one row at a time and extract specific columns of interest.\n",
    "\n",
    "* Use a generator expression to generate an infinite sequence of prime numbers.\n",
    "\n",
    "* Given a large list of numbers, use a generator expression to compute the running average of the numbers as they are processed one at a time.\n",
    "\n",
    "* Use a generator expression to find the largest palindrome number in a large list of numbers.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use a generator expression to read the file one row at a time and filter the rows based on a certain condition.\n",
    "\n",
    "* Use a generator expression to generate an infinite sequence of Fibonacci numbers.\n",
    "\n",
    "* Given a large list of strings, use a generator expression to extract all the words that contain a specific substring.\n",
    "\n",
    "* Use a generator expression to find the longest palindrome word in a large list of strings.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use a generator expression to read the file one row at a time and compute the sum of a specific column.\n",
    "\n",
    "* Use a generator expression to generate an infinite sequence of random numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwUt99GLsr9-"
   },
   "source": [
    "**Asynchronous Programming for Data Processing:**\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Asynchronous programming is a programming paradigm that allows you to run multiple tasks simultaneously and switch between them as needed. \n",
    "* This is important in data science and machine learning when dealing with large datasets or computationally expensive operations. \n",
    "* Asynchronous programming can help to reduce the time required to complete data processing tasks.\n",
    "\n",
    "To expand on this, asynchronous programming is a programming paradigm that allows you to write code that runs non-blocking, meaning that the code can continue to execute while waiting for other tasks to complete. This is achieved through the use of coroutines and event loops, which allow you to run multiple tasks simultaneously and switch between them as needed.\n",
    "\n",
    "Asynchronous programming is particularly useful in data science and machine learning when dealing with large datasets or computationally expensive operations. By allowing tasks to run simultaneously and switch between them as needed, asynchronous programming can help to reduce the time required to complete data processing tasks.\n",
    "\n",
    "Here are a few examples of how asynchronous programming can be used in data science:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O-9xONRs6ZV"
   },
   "source": [
    "**Reading Large Files**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "When reading large files, you can use asynchronous programming to read the file in chunks and process each chunk as it is read. This can help to reduce the memory required to read the file, as well as speed up the processing time. For example, you can use the aiofiles library to read a large file asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "miptMa3RtEXa"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiofiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiofiles\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aiofiles'"
     ]
    }
   ],
   "source": [
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "async def read_file():\n",
    "    async with aiofiles.open('large_file.csv') as f:\n",
    "        async for line in f:\n",
    "            # process the line here\n",
    "            pass\n",
    "\n",
    "asyncio.run(read_file())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kDKtj4wtO0o"
   },
   "source": [
    "**Parallel Processing**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Asynchronous programming can be used to run multiple data processing tasks simultaneously, which can help to speed up the processing time. For example, you can use the asyncio library to run multiple tasks in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5he-ZCKAtSAX"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_data(data):\n",
    "    # process the data here\n",
    "    pass\n",
    "\n",
    "async def run_tasks():\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    tasks = [asyncio.create_task(process_data(d)) for d in data]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "asyncio.run(run_tasks())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdBeipJitVwR"
   },
   "source": [
    "**Web Scraping**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "When web scraping, you can use asynchronous programming to make multiple HTTP requests simultaneously and process the responses as they are received. This can help to speed up the scraping process and reduce the time required to retrieve the data. For example, you can use the aiohttp library to make HTTP requests asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bg4ht7wftZ91"
   },
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch_data(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def run_tasks():\n",
    "    urls = ['http://example.com', 'http://google.com', 'http://bing.com']\n",
    "    tasks = [asyncio.create_task(fetch_data(url)) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    # process the results here\n",
    "\n",
    "asyncio.run(run_tasks())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFktkkuhwxxP"
   },
   "source": [
    "**Higher-Order Functions for Parallelization and Distributed Computing:**\n",
    "---\n",
    "\n",
    "\n",
    "* Higher-order functions are functions that take other functions as arguments or return functions as results. \n",
    "* This is important in data science and machine learning for parallelization and distributed computing. \n",
    "* Higher-order functions can be used to abstract away the details of parallelization and distribute computations across multiple cores or nodes in a cluster.\n",
    "\n",
    "To expand on this, higher-order functions are functions that take other functions as arguments or return functions as results. In Python, functions are first-class objects, which means that they can be treated like any other object in the language, including being passed as arguments to other functions or returned as results.\n",
    "\n",
    "Higher-order functions are particularly useful in data science and machine learning for parallelization and distributed computing. By abstracting away the details of parallelization, higher-order functions can simplify the process of distributing computations across multiple cores or nodes in a cluster.\n",
    "\n",
    "Here are a few examples of how higher-order functions can be used in data science:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gYbMO03w-Yl"
   },
   "source": [
    "**Parallel Processing with Map**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The map() function can be used to process data in parallel across multiple cores. For example, you can use the multiprocessing module in Python to parallelize the processing of the Titanic dataset by splitting it into chunks and processing each chunk in a separate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kiHbF1pqxC7z"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m titanic \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitanic.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define a function to process a chunk of the data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_chunk\u001b[39m(chunk):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Perform some processing on the chunk of data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Define a function to process a chunk of the data\n",
    "def process_chunk(chunk):\n",
    "    # Perform some processing on the chunk of data\n",
    "    ...\n",
    "\n",
    "# Split the data into chunks\n",
    "n_chunks = 4\n",
    "chunks = [titanic[i:i + len(titanic) // n_chunks] for i in range(0, len(titanic), len(titanic) // n_chunks)]\n",
    "\n",
    "# Create a process pool and apply the processing function to each chunk in parallel\n",
    "with Pool(n_chunks) as pool:\n",
    "    results = pool.map(process_chunk, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKtNT7RlxFUF"
   },
   "source": [
    "**Distributed Processing with Dask**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The dask library can be used to process data in a distributed environment across multiple nodes in a cluster. For example, you can use dask.dataframe to load the Titanic dataset and perform distributed processing using Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3u56yFIxJZf"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the Titanic dataset using Dask\n",
    "titanic = dd.read_csv('titanic.csv')\n",
    "\n",
    "# Define a function to process the data\n",
    "def process_data(data):\n",
    "    # Perform some processing on the data\n",
    "    ...\n",
    "\n",
    "# Apply the processing function to the data in parallel using Dask\n",
    "results = titanic.map_partitions(process_data).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSDdcmp0xLD9"
   },
   "source": [
    "**Concurrent Processing with Futures**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The concurrent.futures module can be used to process data concurrently across multiple threads or processes. For example, you can use the ProcessPoolExecutor class to parallelize the processing of the Titanic dataset across multiple processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNYEVF91xcy4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Define a function to process the data\n",
    "def process_data(data):\n",
    "    # Perform some processing on the data\n",
    "    ...\n",
    "\n",
    "# Create a process pool and apply the processing function to the data in parallel\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_data, titanic.to_dict('records')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRX9AaYLxidz"
   },
   "source": [
    "These are just a few examples of how higher-order functions can be used in data science to parallelize and distribute computations across multiple cores or nodes in a cluster. By abstracting away the details of parallelization, higher-order functions can make it easier to work with large datasets and complex computations in data science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jDfRXz38fDA"
   },
   "source": [
    "**Exercises for Higher-Order Functions for Parallelization and Distributed Computing:**\n",
    "\n",
    "* Given a large list of numbers, use the map() function and parallel processing to compute the square of each number.\n",
    "\n",
    "* Given a large list of strings, use the filter() function and parallel processing to extract all the strings that contain a specific substring.\n",
    "\n",
    "* Given a large list of matrices, use the reduce() function and parallel processing to compute the product of all the matrices.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use the map() function and parallel processing to extract specific columns of interest and store them in a new file.\n",
    "\n",
    "* Given a large list of dictionaries, use the filter() function and parallel processing to extract all the dictionaries that have a specific key-value pair.\n",
    "\n",
    "* Given a large list of images, use the map() function and parallel processing to resize each image to a specific size.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use the reduce() function and parallel processing to compute the sum of a specific column.\n",
    "\n",
    "* Given a large list of strings, use the map() function and parallel processing to compute the Levenshtein distance between each pair of strings.\n",
    "\n",
    "* Given a large list of text files, use the map() function and parallel processing to count the occurrences of a specific word in each file.\n",
    "\n",
    "* Given a large list of matrices, use the filter() function and parallel processing to extract all the matrices that have a specific determinant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW-qaFXNxkbW"
   },
   "source": [
    "**Functional Programming Paradigms for Data Pipelines:**\n",
    "---\n",
    "\n",
    "\n",
    "* Functional programming is a programming paradigm that emphasizes the use of pure functions, immutable data structures, and functional composition. \n",
    "* This is important in data science and machine learning to build scalable and maintainable data pipelines. \n",
    "* Functional programming can help to reduce the complexity of data processing pipelines and improve the reliability of data analysis\n",
    "\n",
    "To expand on this, functional programming is a programming paradigm that emphasizes the use of pure functions, immutable data structures, and functional composition. Pure functions are functions that do not have side effects and always return the same output for a given input. Immutable data structures are data structures that cannot be modified once they are created. Functional composition is the process of combining multiple functions into a single function.\n",
    "\n",
    "Functional programming is particularly useful in data science and machine learning to build scalable and maintainable data pipelines. By using pure functions and immutable data structures, functional programming can help to reduce the complexity of data processing pipelines and improve the reliability of data analysis.\n",
    "\n",
    "Here are a few examples of how functional programming can be used in data science using **Titanic Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCj6uFAHyyAR"
   },
   "source": [
    "**Map and Reduce Operations**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Map and reduce operations can be used to process data in the Titanic dataset. For example, you can use map to transform the data, and reduce to aggregate the data. Here's an example of using map and reduce to calculate the average age of male passengers who survived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VcKqLfH5y04i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = pd.read_csv('tested.csv')\n",
    "\n",
    "# Filter the dataset to include only male passengers who survived\n",
    "male_survivors = titanic[(titanic.Sex == 'male') & (titanic.Survived == 1)]\n",
    "\n",
    "# Use map to extract the age of each passenger\n",
    "ages = male_survivors['Age'].tolist()\n",
    "\n",
    "if len(ages) > 0:\n",
    "    # Use reduce to calculate the average age\n",
    "    average_age = sum(ages) / len(ages)\n",
    "else:\n",
    "    average_age = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgEcbIYy5LE"
   },
   "source": [
    "**Function Composition**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Function composition can be used to simplify the code and reduce the number of intermediate variables. For example, you can use function composition to filter the dataset, extract the age of each passenger, and calculate the average age in a single expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X8pqesdry9AV"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = pd.read_csv('tested.csv')\n",
    "\n",
    "# Filter the dataset to include only male passengers who survived\n",
    "filtered_titanic = titanic[(titanic.Sex == 'male') & (titanic.Survived == 1)]\n",
    "\n",
    "if len(filtered_titanic) > 0:\n",
    "    # Use function composition to extract the age of each passenger and calculate the average age\n",
    "    average_age = reduce(lambda acc, x: acc + x, map(lambda x: x['Age'], filtered_titanic.to_dict('records'))) / len(filtered_titanic)\n",
    "else:\n",
    "    average_age = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGBC0gOpy_YR"
   },
   "source": [
    "**Immutable Data Structures**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Immutable data structures can be used to ensure that the data remains consistent and prevent unintended side effects. For example, you can use the NamedTuple class in Python to create immutable data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o5rv0wVlzDuR"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Passenger(NamedTuple):\n",
    "    name: str\n",
    "    age: int\n",
    "    sex: str\n",
    "    survived: bool\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = pd.read_csv('tested.csv')\n",
    "\n",
    "# Create a list of Passenger objects\n",
    "passengers = [Passenger(name=row['Name'], age=row['Age'], sex=row['Sex'], survived=row['Survived']) for index, row in titanic.iterrows()]\n",
    "\n",
    "# Use the passengers list to perform further processing, without modifying the original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG2cCuru9loR"
   },
   "source": [
    "**Exercises for Functional Programming Paradigms for Data Pipelines:**\n",
    "\n",
    "\n",
    "* Given a large CSV file with millions of rows, use functional programming to read the file one row at a time, apply a series of transformations (e.g. filtering, grouping, aggregation), and write the results to a new file.\n",
    "\n",
    "* Given a large list of strings, use functional programming to extract all the words that contain a specific substring, and then sort the resulting list by word length.\n",
    "\n",
    "* Given a large list of matrices, use functional programming to compute the product of all the matrices, using pure functions and immutable data structures.\n",
    "\n",
    "* Given a large list of dictionaries, use functional programming to extract all the dictionaries that have a specific key-value pair, and then compute the average value of a specific key across all the matching dictionaries.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use functional programming to read the file one row at a time, apply a series of transformations (e.g. filtering, grouping, aggregation), and then visualize the results using a plotting library.\n",
    "\n",
    "* Given a large list of strings, use functional programming to compute the similarity between each pair of strings, using a string distance metric (e.g. Levenshtein distance) and a functional composition of map, filter, and reduce.\n",
    "\n",
    "* Given a large list of images, use functional programming to apply a series of image processing operations (e.g. resizing, cropping, filtering) to each image, and then save the results to a new directory.\n",
    "\n",
    "* Given a large list of text files, use functional programming to count the occurrences of each word across all the files, and then visualize the results using a word cloud library.\n",
    "\n",
    "* Given a large list of matrices, use functional programming to compute the determinant of each matrix, and then filter the resulting list to include only matrices with a specific determinant value.\n",
    "\n",
    "* Given a large CSV file with millions of rows, use functional programming to read the file one row at a time, apply a series of transformations (e.g. filtering, grouping, aggregation), and then perform a statistical analysis on the results using a library like NumPy or SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnnExzRA0VI-"
   },
   "source": [
    "**Pandas and NumPy Libraries for Data Manipulation:**\n",
    "---\n",
    "\n",
    "\n",
    "* Pandas and NumPy are popular Python libraries used for data manipulation and analysis. \n",
    "* This is important in data science and machine learning to work with large datasets efficiently. \n",
    "* Pandas and NumPy provide powerful and efficient data structures and operations for data manipulation and analysis, making them essential tools for any data science or machine learning project.\n",
    "\n",
    "To expand on this, Pandas and NumPy are two of the most widely used Python libraries in data science and machine learning for data manipulation and analysis. Pandas provides high-level data structures and functions for data manipulation and analysis, while NumPy provides low-level data structures and functions for numerical computing.\n",
    "\n",
    "Here are a few examples of how Pandas and NumPy can be used for data manipulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWvGar0y02JP"
   },
   "source": [
    "**Data Cleaning**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pandas can be used to clean and preprocess data, such as removing missing values, filling in missing values, and converting data types. For example, you can use the dropna() function to remove rows with missing values, and the fillna() function to fill in missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UkJ0ymUkw9Jf"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Remove rows with missing values\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Fill in missing values\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Convert data types\n",
    "data['column'] = data['column'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI4wk5ne3MgY"
   },
   "source": [
    "**Data Transformation**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pandas can be used to transform data, such as applying functions to columns or creating new columns based on existing columns. For example, you can use the apply() function to apply a function to a column, and the assign() function to create a new column based on existing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3qYcoBTtNGO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Apply a function to a column\n",
    "data['column'] = data['column'].apply(lambda x: x + 1)\n",
    "\n",
    "# Create a new column based on existing columns\n",
    "data = data.assign(new_column=data['column1'] + data['column2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0OIONlu3Wjh"
   },
   "source": [
    "**Data Aggregation**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pandas can be used to aggregate data, such as grouping data by a column and calculating summary statistics. For example, you can use the groupby() function to group data by a column, and the agg() function to calculate summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXjft3QT35aJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Group the data by a column and calculate the mean of another column\n",
    "grouped_data = data.groupby('column1')['column2'].agg('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ya7N-6i38n7"
   },
   "source": [
    "**Numerical Computing**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "NumPy can be used for numerical computing, such as performing vector and matrix operations. For example, you can use the dot() function to perform matrix multiplication, and the linspace() function to create a range of evenly spaced numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6zv44eh4Acv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create two matrices\n",
    "matrix1 = np.array([[1, 2], [3, 4]])\n",
    "matrix2 = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = np.dot(matrix1, matrix2)\n",
    "\n",
    "# Create a range of evenly spaced numbers\n",
    "numbers = np.linspace(0, 1, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBa-7N0r_ohS"
   },
   "source": [
    "**Exercises  for data transformation, data cleaning, data aggregation and numerical computing**\n",
    "\n",
    "Data Transformation:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given a large list of strings representing dates, use string manipulation functions to transform the dates into a standardized format (e.g. yyyy-mm-dd).\n",
    "* Given a large CSV file with mixed data types, use type casting functions to transform the data into the correct data types (e.g. strings to numbers, dates to datetime objects).\n",
    "* Given a large list of strings, use regular expressions to extract specific patterns of interest and transform them into new strings.\n",
    "* Given a large CSV file with multiple columns, use the pivot() function in Pandas to transform the data into a pivot table format.\n",
    "* Given a large list of strings representing geographic coordinates, use math functions to transform the coordinates into a different projection or coordinate system.\n",
    "* Given a large CSV file with multiple columns, use the melt() function in Pandas to transform the data into a long format.\n",
    "* Given a large list of strings, use the split() function to separate the strings into multiple parts and transform them into nested data structures.\n",
    "* Given a large CSV file with multiple columns, use the stack() function in Pandas to transform the data into a stacked format.\n",
    "* Given a large list of strings, use the join() function to concatenate the strings into longer strings, with optional separator characters.\n",
    "* Given a large CSV file with missing values, use the fillna() function in Pandas to transform the missing values into a specific value or a value calculated from the other values.\n",
    "\n",
    "Data Cleaning:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given a large CSV file with duplicate rows, use the drop_duplicates() function in Pandas to remove the duplicates based on specific columns.\n",
    "* Given a large list of strings, use the strip() function to remove leading and trailing whitespace from the strings.\n",
    "* Given a large CSV file with missing values, use the dropna() function in Pandas to remove the rows with missing values based on specific columns.\n",
    "* Given a large list of strings, use the lower() function to convert all the strings to lowercase.\n",
    "* Given a large CSV file with inconsistent values in a specific column, use the replace() function in Pandas to replace the inconsistent values with consistent values.\n",
    "* Given a large list of strings, use the replace() function to replace specific substrings with other substrings.\n",
    "* Given a large CSV file with invalid values in a specific column, use the astype() function in Pandas to convert the column to a numeric data type, and handle the invalid values with the errors parameter.\n",
    "* Given a large list of strings, use the translate() function to remove specific characters from the strings.\n",
    "* Given a large CSV file with multiple columns, use the apply() function in Pandas to apply a custom function to transform the data in specific columns.\n",
    "* Given a large list of strings, use the split() function to separate the strings into multiple parts, and then use the isdigit() function to remove non-numeric parts from the strings.\n",
    "\n",
    "Data Aggregation:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given a large CSV file with multiple rows and columns, use the groupby() function in Pandas to aggregate the data based on specific columns and calculate summary statistics.\n",
    "* Given a large list of dictionaries, use the reduce* () function and functional programming to compute the overall sum or average of a specific key across all the dictionaries.\n",
    "* Given a large CSV file with multiple columns, use the crosstab() function in Pandas to calculate cross-tabulations of the data based on specific columns.\n",
    "* Given a large list of tuples representing time series data, use the groupby() function in Pandas to aggregate the data based on specific time periods (e.g. days, weeks, months) and calculate summary statistics.\n",
    "* Given a large CSV file with multiple columns, use the pivot_table() function in Pandas to aggregate the data based on specific columns and calculate summary statistics.\n",
    "* Given a large list of dictionaries, use the map() function and functional programming to transform the data and then use the reduce() function to aggregate the data based on specific keys and calculate summary statistics.\n",
    "* Given a large CSV file with multiple columns, use the agg() function in Pandas to apply multiple aggregation functions to specific columns and calculate summary statistics.\n",
    "* Given a large list of tuples representing time series data, use the resample() function in Pandas to resample the data to a different frequency and aggregate the data based on specific time periods.\n",
    "* Given a large CSV file with multiple columns, use the rolling() function in Pandas to calculate rolling window statistics (e.g. moving average, standard deviation) based on specific columns.\n",
    "* Given a large list of dictionaries, use the groupby() function and functional programming to group the data based on multiple keys and calculate summary statistics for each group.\n",
    "\n",
    "\n",
    "Numerical Computing:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Given a large list of numbers, use the sum() function to calculate the sum of the numbers.\n",
    "* Given a large list of numbers, use the max() and min() functions to calculate the maximum and minimum values of the numbers.\n",
    "* Given a large list of numbers, use the mean() function to calculate the arithmetic mean of the numbers.\n",
    "* Given a large list of numbers, use the median() function to calculate the median value of the numbers.\n",
    "* Given a large list of numbers, use the std() function to calculate the standard deviation of the numbers.\n",
    "* Given a large list of numbers, use the var() function to calculate the variance of the numbers.\n",
    "* Given a large list of numbers, use the percentile() function in NumPy to calculate the percentile values of the numbers.\n",
    "* Given a large list of numbers, use the histogram() function in NumPy to compute the histogram of the numbers and visualize the results.\n",
    "* Given a large CSV file with multiple numeric columns, use the corr() function in Pandas to compute the pairwise correlation coefficients between the columns.\n",
    "* Given a large list of numbers, use the filter() function and functional programming to remove outliers from the data and then compute summary statistics on the remaining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV4r5H91A6Tq"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
